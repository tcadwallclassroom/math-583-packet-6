---
title: "Packet 6 - Confidence Intervals"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Todd CadwalladerOlsker"
date: "*Last updated:* `r Sys.Date()`"
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(rmdformats)
library(openintro)
library(tidyverse)
library(gghighlight)
library(formatR)
knitr::opts_chunk$set(echo = T, cache = T, eval = T, cache.lazy = F, warning = F)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=F)
options(scipen = 999)
```

## Introducing confidence intervals

Whenever we derive a sample statistic, that statistic is only an *estimate* of the population parameter. The value of the parameter might be slightly higher or slightly lower than that of the statistic. A *confidence interval* gives us a range of values where we beleive the true parameter value is likely to be, with a certain "level of confidence". 

## Confidence Interval for a Single Proportion



For the next several examples, we are going to make use of the [General Social Survey](http://gss.norc.org/) data. In order to make this easier on us, we are going to install the `gssr` package; unfortunately, this is a bit more involved than most packages. Run the following in RStudio:

```{r gssr}
#The following only needs to be run once:

#install.packages("drat")
#library(drat)
#drat::addRepo("kjhealy")
#install.packages("gssr")
library(gssr)
```

You can see some examples using the `gssr` package at the [Overview of the `gssr` package.](https://kjhealy.github.io/gssr/articles/overview.html) First, we'll recreate the data from 2010 that we saw in the `openintro` package:

```{r gssr openintro}
data(gss_doc)
#View(gss_doc) for information on all the variables.
#data(gss_all) will load the entire (gigantic) data set.

num_vars <- c("hrsrelax", "mntlhlth", "hrs1")
cat_vars <- c("degree", "grass")
my_vars <- c(num_vars, cat_vars)

gss10 <- gss_get_yr(2010)
gss10 <- gss10 %>% 
  select(all_of(my_vars)) %>% 
  mutate(
    # Convert all missing to NA
    across(everything(), haven::zap_missing),
    # Make all categorical variables factors and relabel nicely
    across(all_of(cat_vars), forcats::as_factor)
    )
```

Now, we can view the data on the `grass` variable just like we did in the previous packet:

```{r tables}
grass10_table <- table(gss10$grass)
grass10_table
grass10_props <- grass10_table %>% prop.table()
grass10_props
```

We can see that our *point estimate* based on this sample of 1259 people is that 47.9% of the population (in 2010) supported making marijuana legal. 

The nice this about the `gssr` package is that we can easily choose whatever years and variables we want. For example, before continuing, find a point estimate for the percentage of Americans that supported legalizing marijuana in 2018 (the most recent year in which this question was asked).

Now, the question is: how good is this estimate? This is where a confidence interval comes in. 

With a "large enough" sample size (meaning that the success-failure conditions have been met), the distribution of the sample proportion is approximately normal, and the standard error can be calculated:

\[SE = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\]

In this example, the success-failure conditions have, in fact, been met, so we calculate

```{r gss se}
p_hat <- grass10_props["legal"]
n <- grass10_table["legal"] + grass10_table["not legal"]
se <- sqrt(p_hat * (1 - p_hat) / n)
```

For a 95% confidence interval, we wish to find the "middle 95%" of the normal curve:

```{r gss normTail}
qnorm(c(0.025,0.975),0,1)
normTail(0, 1, M = qnorm(c(0.025,0.975),0,1), 
        col = "forestgreen", xLab = "symbol")

c(lower = p_hat - 1.96 * se, upper = p_hat + 1.96 * se)

qnorm(c(0.025,0.975),p_hat,se)
normTail(p_hat, se, M = qnorm(c(0.025,0.975),p_hat,se),
        col = "forestgreen")
```

What does "95%" mean in this context? We are "95% confident", but 95% of *what*?

Here's a nice simulation that can answer that question for us: Suppose the true proportion of support for legalizing marijuana in 2018 is 65%. If we took many samples of 1447 people, what kind of confidence intervals would we get?

```{r conf int sim, eval = F}
p <- 0.65
n <- 1447
simulations <- 10000
success <- "legal"
responses <- c("legal", "not legal")

p_hat <- numeric(simulations)
se <- numeric(simulations)
lower_bound <- numeric(simulations)
upper_bound <- numeric(simulations)

for(i in 1:simulations){
  sample <- sample(responses, size = n, prob = c(p,(1-p)), replace = T)
  p_hat[i] <-  sum(sample == success)/n
  se[i] <- sqrt(p_hat[i] * (1 - p_hat[i]) / n)
  lower_bound[i] <- p_hat[i] - 1.96 * se[i]
  upper_bound[i] <- p_hat[i] + 1.96 * se[i]
}

conf_intervals <- data.frame(p_hat, 
                             se, 
                             lower_bound, 
                             upper_bound)
conf_intervals <- conf_intervals %>% 
  mutate(has_p = (p > lower_bound & p < upper_bound))

sum(conf_intervals$has_p == TRUE)/simulations

```

What does this simulation tell us about the meaning of the 95% confidence interval?

## Margin of Error

Another example: A recent poll (conducted March 4-6, 2022) by Quinnipiac University surveyed 1,374 American adults, and reported that 71% of American adults would support a ban on Russian oil even if it meant higher gas prices [See press release here.](https://poll.qu.edu/poll-release?releaseid=3838). The fine print says that the poll has a margin of error of $\pm 2.6$ percentage points. 

a. Find the standard error of this poll, based on the normal distribution, and use it to calculate a 95% confidence interval. 
b. Does your calculation match the $\pm 2.6$ "margin of error" stated by Quinnipiac? What might be the difference between your calculation and that of Quinnipiac?

## The Bootstrap

The *bootstrap* is a way of approximating the variation in our point estimation using our sample. The idea is this: In the 2010 GSS, we had 603 respondents who supported legalizing marijuana out of 1259 total, for an estimated proportion of 0.479. If this estimate is accurate, we can think of 1259 marbles in a bag, where 603 of them are red. The variation can be simulated by pulling out 1259 marbles *with replacement* many times -- in essence, giving us many simulated sample point estimates. We can then get a confidence interval by looking at the "middle 95%" of our trials.

```{r bootstrap}
n <- grass10_table["legal"] + grass10_table["not legal"]
simulations <- 10000
data <- (gss10 %>% drop_na(grass))$grass
success <- "legal"
p_hat <- numeric(simulations)

for(i in 1:simulations){
  sample <- sample(data, n, replace = T)
  p_hat[i] <-  sum(sample == success)/n
}

results <- data.frame(p_hat)

quantile(results$p_hat, probs = c(0.025, 0.975))
lower_bound <- quantile(results$p_hat, probs = c(0.025, 0.975))["2.5%"]
upper_bound <- quantile(results$p_hat, probs = c(0.025, 0.975))["97.5%"]

results %>% ggplot(aes(x = p_hat)) +
  geom_bar(width = 1/n,  
           color = "black", 
           fill = "forestgreen") +
  gghighlight(quantile(results$p_hat, 0.975) >= p_hat &
              quantile(results$p_hat, 0.025) <= p_hat,
              unhighlighted_params = list(
                color = "grey",
                fill = "forestgreen", 
                alpha = 0.25)) +
  labs(
    title = paste(simulations, "bootstrap simulations of the population that supported legalization of marijuana"),
    subtitle = "Based on 2010 GSS data",
    x = "Sample Proportion",
    y = "Number of Simulations"
  ) +
  scale_x_continuous(breaks = seq(0.44, 0.56, 0.02))

```

We can see that 95% of our data lies between `r lower_bound` and `r upper_bound`, making this a good 95% confidence interval for the proportion that favored legalizing marijuana in 2010. 

## The `infer` package

The `infer` package can make our randomization-based hypothesis tests and bootstrap confidence intervals a little simpler to run. Install and run the `infer` package before continuing:

```{r inferload}
# install.packages("infer")
library(infer)
```

Let's look at the `gss10` data we found earlier. The support for legalizing marijuana was a bit under 48%. Using the `infer` package:

```{r infer stat}
p_hat <- gss10 %>%
  specify(response = grass, success = "legal") %>%
  calculate(stat = "prop")
p_hat
```

Does this indicate that a minority of the 2010 American population supports legalizing marijuana? A *hypothesis test* compares this observation from the sample to a hypothetical distribution based on the null hypothesis. The `infer` package creates the hypothetical distribution using the following commands:

```{r hypo}
null_dist <- gss10 %>%
  specify(response = grass, success = "legal") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 10000, type = "draw") %>%
  calculate(stat = "prop")
```

Now, we can find the number of simulated values more extreme than that of our test statistic:

```{r infer pvalue}
p_value <- null_dist %>%
  get_p_value(obs_stat = p_hat, direction = "less")
p_value
```

We get a p-value of `r p_value`, not quite small enough to reject the null hypothesis at an $\alpha = 0.05$ level. We can visualize this on a graph:

```{r infer vis}
null_dist %>% 
  visualize() +
  shade_p_value(obs_stat = p_hat, direction = "less") +
  labs(
    x = "Simulated proportion",
    y = "Number of simulations"
  )
```

For a bootstrap confidence interval, we can use the same "sample with replacement" technique as before. The `infer` package calls the "bootstrap" type when generating a distribution:

```{r infer bootstrap}
boot_dist <- gss10 %>%
  specify(response = grass, success = "legal") %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "prop")


# Use the bootstrap distribution to find a confidence interval:

percentile_ci <- get_ci(boot_dist, level = 0.95)
percentile_ci

# Visualizing the observed statistic alongside the distribution:

boot_dist %>% 
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci) +
  labs(
    x = "Bootstrapped Proportion",
    y = "Number of simulations"
  )
```



## Math 120 Monkey Investigation

In Math 120, students are given of an experiment: Two potatoes, one cooked and one raw, are placed in front of a monkey, and the money is allowed to choose one. The experiment is repeated 12 times, with the following results:

|Cooked|Raw|
|---|---|
|9|3|

The question is whether or not the monkey prefers one potato over the other. Students are asked to determine whether or not this data suggests that the monkey exhibits a preference for cooked potatoes, or if it is likely that the monkey is just choosing its potato randomly.

1. Conduct a randomization-based hypothesis test using the null hypothesis that $p=0.5$ (that is, the null hypothesis is that the monkey is choosing randomly), and alternative hypothesis $p \ne 0.5$ (that is, a two-tailed hypothesis test). What p-value do you get?
2. Find a confidence interval for the proportion of cooked potatoes that the monkey chooses using the bootstrap method. 

I've posted the Math 120 worksheets (courtesy of Dr. Wynne) on our Canvas page, you can see the full investigation (including the "six-step process" used in Math 120) there.

## To Turn In

Our next topic will be to compare two proportions from different populations. For example, we might compare the level of support for legalizing marijuana for those with a college degree to those without in 2018.

1. Using the `gss18` dataset, collapse the `degree` variable into a binary "degree/no degree" variable that we will call `college` using the following code:

```{r to turn in 1, eval = FALSE}
gss18 <- gss18 %>% 
  mutate(
    college = recode(degree,
                   "lt high school" = "no degree",
                   "high school" = "no degree",
                   "junior college" = "degree",
                   "bachelor" = "degree",
                   "graduate" = "degree"
    )
  )
```

2. Run the following commands:

`table(gss18$college, gss18$grass) %>% addmargins()`
`table(gss18$college, gss18$grass) %>% prop.table(margin = 1)`

Using an eyeball estimate, does it appear that there is a difference in the level of support for each group?

2. Create a 95% confidence interval using the standard error method for the proportion of each group who support legalization of marijuana. (You may wish to use `filter` to analyze each group separately.)

3. Create a 95% confidence interval using the bootstrap method (10000 simulations) for the proportion of each group who support legalization of marijuana.

4. Do these confidence intervals overlap? If so, or if not, what does that mean?

5. Now, re-do parts 2 and 3 to create a 99% confidence interval in each case. If 1.96 was the number of standard deviations (in each direction) needed for a 95% confidence interval, what is the number of standard deviations we need (again, in each direction) for a 99% interval?